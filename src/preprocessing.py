# -*- coding: utf-8 -*-
"""SP3-RiskManagement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VDzG_3AZSOSoJKZFErDvOtPd_LBko2OL

GOD-Caterpillar

## Libraries and tables loading
"""

import pandas as pd
import numpy as np
from datetime import datetime
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew, kurtosis, normaltest

account_path='/content/drive/MyDrive/SP3-Caterpillar/Account.csv' # account information such as ID, location and creation date
disposition_path='/content/drive/MyDrive/SP3-Caterpillar/Disposition.csv' # the type or disposition (owner/user) and Client ID
client_path='/content/drive/MyDrive/SP3-Caterpillar/Client.csv' # client information such as ID, birth date and gender
demograph_path='/content/drive/MyDrive/SP3-Caterpillar/Demograph.csv' # demographic info such as district, salary and entrepreneurs rate
transaction_path='/content/drive/MyDrive/SP3-Caterpillar/Transactions.csv' # transaction information such as amount, date and type
creditcard_path='/content/drive/MyDrive/SP3-Caterpillar/CreditCard.csv' # info such as ID, disposition ID, type and issued date
permanentorder_path='/content/drive/MyDrive/SP3-Caterpillar/PermanentOrder.csv'# info such as bank and account which the operation goes
loan_path='/content/drive/MyDrive/SP3-Caterpillar/Loan.csv' # info such as ID, account, date, amount, duration, payments and status

account_df = pd.read_csv(account_path, sep=';')
disposition_df = pd.read_csv(disposition_path, sep=';')
client_df = pd.read_csv(client_path, sep=';')
demograph_df = pd.read_csv(demograph_path, sep=';')
transaction_df = pd.read_csv(transaction_path)
creditcard_df = pd.read_csv(creditcard_path, sep=';')
permanentorder_df = pd.read_csv(permanentorder_path, sep=';')
loan_df = pd.read_csv(loan_path, sep=';')

# Set formatting
pd.options.display.float_format = '{:,.2f}'.format

"""# DATA UNDERSTANDING AND PREPROCESSING

## 0. Exploring datasets

### 1. Account
"""

account_df.info()

account_df.describe()

account_df.head(10)

"""### 2. Disposition"""

disposition_df.info()

disposition_df.describe()

disposition_df.head(10)

"""### 3. Client"""

client_df.info()

client_df.describe()

client_df.head(10)

"""### 4. Demograph"""

demograph_df.info()

demograph_df.describe()

demograph_df.head(10)

"""### 5. Transactions"""

transaction_df.head(10)

transaction_df.info()

transaction_df.describe()

"""### 6. Credit card"""

creditcard_df.head(10)

creditcard_df.info()

creditcard_df.describe()

"""### 7. Permanent order"""

permanentorder_df.head(10)

permanentorder_df.info()

permanentorder_df.describe()

"""### 8. Loan"""

loan_df.head(10)

loan_df.info()

loan_df.describe()

"""## 1. Data Quality Analysis

### Data conversion
"""

# We need to converts dates to proper datetime format.

# Convert transaction dates (assuming they're in the same format)
account_df['date'] = pd.to_datetime(account_df['date'].astype(str), format='%y%m%d')

# Convert transaction dates (assuming they're in the same format)
transaction_df['date'] = pd.to_datetime(transaction_df['date'].astype(str), format='%y%m%d')

# Convert loan dates (format YYMMDD)
loan_df['date'] = pd.to_datetime(loan_df['date'].astype(str), format='%y%m%d')

# Group data by status and calculate the sum, min, max, and average for amount, duration, and payments
loan_stats = loan_df.groupby('status').agg(
    {'amount': ['sum', 'min', 'max', 'mean'],
     'duration': ['sum', 'min', 'max', 'mean'],
     'payments': ['sum', 'min', 'max', 'mean']}
)

# Display the results
loan_stats

# Filter the loan_df DataFrame to include only rows where the account_id is 11013
account_id_11013_payments = loan_df[loan_df['account_id'] == 11013]

# Display the filtered DataFrame, showing only payments for account_id 11013
print(account_id_11013_payments)

# Calculate and print the number of payments associated with account_id 11013
num_payments = len(account_id_11013_payments)
print(f"\nNumber of payments for account_id 11013: {num_payments}")

# prompt: How to know if there are customers with more than 1 payment, please?

# Count the occurrences of each account_id in the loan_df DataFrame
account_payment_counts = loan_df['account_id'].value_counts()

# Filter the account_payment_counts Series to include only accounts with more than one payment
accounts_with_multiple_payments = account_payment_counts[account_payment_counts > 1]

# Print the account IDs with more than one payment
print("Accounts with more than one payment:")
print(accounts_with_multiple_payments)

# Check if there are any such accounts
if len(accounts_with_multiple_payments) > 0:
    print("\nYes, there are customers with more than 1 payment.")
else:
    print("\nNo, there are no customers with more than 1 payment.")

# prompt: How to know if there are customers with more than 1 loan, please?

# Count the occurrences of each account_id in the loan_df DataFrame
account_loan_counts = loan_df['account_id'].value_counts()

# Filter the account_payment_counts Series to include only accounts with more than one loan
accounts_with_multiple_loans = account_loan_counts[account_loan_counts > 1]

# Print the account IDs with more than one payment
print("Accounts with more than one loan:")
print(accounts_with_multiple_loans)

# Check if there are any such accounts
if len(accounts_with_multiple_loans) > 0:
    print("\nYes, there are customers with more than 1 loan.")
else:
    print("\nNo, there are no customers with more than 1 loan.")

"""### Preprocessing"""

# We need to create a class that will handle all our preprocessing.
# It initializes three empty dataframes that we'll use throughout the process.
class LoanPreprocessor:
    def __init__(self):
        self.loan_df = None # Why are not using the other 5 dataframes?
        self.account_df = None
        self.transaction_df = None

    def read_data(self, loan_path, account_path, transaction_path):
        """
        Read and initialize all necessary datasets
        """
        # Read CSVs
        self.loan_df = loan_df # pd.read_csv(loan_path, sep=';')
        self.account_df = account_df # pd.read_csv(account_path, sep=';')
        self.transaction_df = transaction_df #pd.read_csv(transaction_path)

    def create_target_variable(self):
        """
        Create binary target variable for default prediction
        B = Default, Others = Non-default
        """
        self.loan_df['default'] = (self.loan_df['status'] == 'B').astype(int)

    def calculate_loan_features(self):
        """
        Calculate basic loan-related features
        """
        # Calculate loan to payment ratio
        self.loan_df['loan_payment_ratio'] = self.loan_df['amount'] / (self.loan_df['payments'] * self.loan_df['duration'])

        # Calculate monthly payment
        self.loan_df['monthly_payment'] = self.loan_df['amount'] / self.loan_df['duration']

    def calculate_account_features(self, days_lookback=90):
        """
        Calculate account-related features using transaction history
        """
        features = []

        for _, loan in self.loan_df.iterrows():
            loan_date = loan['date']
            account_id = loan['account_id']

            # Get transactions for this account before loan date
            account_txns = self.transaction_df[
                (self.transaction_df['account_id'] == account_id) &
                (pd.to_datetime(self.transaction_df['date']) < loan_date)
            ]

            # Calculate features
            avg_balance = account_txns['balance'].mean() if not account_txns.empty else 0
            min_balance = account_txns['balance'].min() if not account_txns.empty else 0
            balance_volatility = account_txns['balance'].std() if not account_txns.empty else 0

            features.append({
                'loan_id': loan['loan_id'],
                'avg_balance': avg_balance,
                'min_balance': min_balance,
                'balance_volatility': balance_volatility
            })

        # Convert to DataFrame and merge with loan data
        account_features = pd.DataFrame(features)
        self.loan_df = self.loan_df.merge(account_features, on='loan_id', how='left')

    def preprocess(self, loan_path, account_path, transaction_path):
        """
        Execute full preprocessing pipeline
        """
        print("Reading data...")
        self.read_data(loan_path, account_path, transaction_path)

        print("Creating target variable...")
        self.create_target_variable()

        print("Calculating loan features...")
        self.calculate_loan_features()

        print("Calculating account features...")
        self.calculate_account_features()

        print("Preprocessing complete!")
        return self.loan_df

preprocessor = LoanPreprocessor()
processed_data = preprocessor.preprocess(
    loan_path='/content/drive/MyDrive/SP3-Caterpillar/Loan.csv',
    account_path='/content/drive/MyDrive/SP3-Caterpillar/Account.csv',
    transaction_path='/content/drive/MyDrive/SP3-Caterpillar/Transactions.csv'
)

# prompt: I want to see all the transactions (in the transactions table) before 1993-07-05 for account ID 1787 and calculate the average of the balance feature and the standard deviation for the balance.

# Filter transactions before 1993-07-05 for account ID 1787
filtered_transactions = transaction_df[
    (transaction_df['account_id'] == 1787) &
    (transaction_df['date'] < datetime(1993, 7, 5))
]

# Calculate the average balance
average_balance = filtered_transactions['balance'].mean()

# Calculate the standard deviation of the balance
std_dev_balance = filtered_transactions['balance'].std()

# Print the results
print(f"Average balance: {average_balance}")
print(f"Standard deviation of balance: {std_dev_balance}")
print(filtered_transactions)

# Check the results
print("\nProcessed data shape:", processed_data.shape)
print("\nFirst few rows:")
processed_data.head(5)

# prompt: I want to run the function preprocessor just for two clients the one with loan ID=5314 and another one with the Loan ID = 5316 please

def preprocess_specific_loans(loan_ids):
    """
    Executes the preprocessing pipeline only for specified loan IDs.
    """

    # Create a copy of the original loan DataFrame to avoid modifying the original
    loan_subset = loan_df[loan_df['loan_id'].isin(loan_ids)].copy()

    # Initialize the preprocessor with the subsetted data
    preprocessor = LoanPreprocessor()
    preprocessor.loan_df = loan_subset
    preprocessor.account_df = account_df  # Using the original account DataFrame
    preprocessor.transaction_df = transaction_df # Using the original transaction DataFrame

    # Perform preprocessing steps for the specific loans
    preprocessor.create_target_variable()
    preprocessor.calculate_loan_features()
    preprocessor.calculate_account_features()

    return preprocessor.loan_df

# Example usage:
specific_loan_ids = [5314, 5316]
processed_specific_loans = preprocess_specific_loans(specific_loan_ids)
processed_specific_loans

"""### Basic Analysis"""

def analyze_data_quality(df, table_name):
    """
    Comprehensive data quality analysis of a DataFrame

    Parameters:
    df (pandas.DataFrame): DataFrame to analyze
    table_name (str): Name of the table for reporting

    Returns:
    dict: Dictionary containing all analysis results
    """
    results = {}

    # 1. Basic Information
    results['basic_info'] = {
        'table_name': table_name,
        'rows': len(df),
        'columns': len(df.columns),
        'memory_usage': df.memory_usage(deep=True).sum() / 1024**2,  # In MB
        'duplicate_rows': df.duplicated().sum()
    }

    # 2. Column Analysis
    column_analysis = {}
    for column in df.columns:
        col_stats = {
            # Data type information
            'dtype': str(df[column].dtype),

            # Missing values
            'missing_count': df[column].isnull().sum(),
            'missing_percentage': (df[column].isnull().sum() / len(df)) * 100,

            # Unique values
            'unique_count': df[column].nunique(),
            'unique_percentage': (df[column].nunique() / len(df)) * 100,

            # Basic statistics for numeric columns
            'min': df[column].min() if pd.api.types.is_numeric_dtype(df[column]) else None,
            'max': df[column].max() if pd.api.types.is_numeric_dtype(df[column]) else None,
            'mean': df[column].mean() if pd.api.types.is_numeric_dtype(df[column]) else None,
            'std': df[column].std() if pd.api.types.is_numeric_dtype(df[column]) else None,

            # Sample values
            'sample_values': list(df[column].value_counts().head(5).index),

            # Zero counts for numeric columns
            'zero_count': (df[column] == 0).sum() if pd.api.types.is_numeric_dtype(df[column]) else None,

            # Empty strings for string columns
            'empty_string_count': (df[column] == '').sum() if pd.api.types.is_string_dtype(df[column]) else None
        }

        # Additional checks for string columns
        if pd.api.types.is_string_dtype(df[column]):
            col_stats.update({
                'min_length': df[column].str.len().min(),
                'max_length': df[column].str.len().max(),
                'avg_length': df[column].str.len().mean()
            })

        column_analysis[column] = col_stats

    results['column_analysis'] = column_analysis

    # 3. Correlation Analysis for Numeric Columns
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    if len(numeric_columns) > 1:
        results['correlations'] = df[numeric_columns].corr().to_dict()

    # 4. Print Summary Report
    print(f"\n{'='*50}")
    print(f"DATA QUALITY ANALYSIS: {table_name}")
    print(f"{'='*50}")

    print("\n1. Basic Information:")
    print(f"Rows: {results['basic_info']['rows']:,}")
    print(f"Columns: {results['basic_info']['columns']}")
    print(f"Memory Usage: {results['basic_info']['memory_usage']:.2f} MB")
    print(f"Duplicate Rows: {results['basic_info']['duplicate_rows']}")

    print("\n2. Column Analysis:")
    for column, stats in results['column_analysis'].items():
        print(f"\n{column}:")
        print(f"  - Type: {stats['dtype']}")
        print(f"  - Missing: {stats['missing_count']:,} ({stats['missing_percentage']:.2f}%)")
        print(f"  - Unique: {stats['unique_count']:,} ({stats['unique_percentage']:.2f}%)")
        print(f"  - Sample Values: {stats['sample_values']}")

        if pd.api.types.is_numeric_dtype(df[column]):
            print(f"  - Range: [{stats['min']}, {stats['max']}]")
            print(f"  - Mean: {stats['mean']}")
            print(f"  - Std: {stats['std']}")

        if pd.api.types.is_string_dtype(df[column]):
            print(f"  - String Lengths: Min={stats['min_length']}, Max={stats['max_length']}, Avg={stats['avg_length']:.2f}")

    return results

# Example usage:
# df = pd.read_csv('Loan.csv', sep=';')
analysis_results = analyze_data_quality(loan_df, 'Loan')

"""### Executing the function"""

analysis_results = analyze_data_quality(account_df, 'Account')

analysis_results = analyze_data_quality(disposition_df, 'Disposition')

analysis_results = analyze_data_quality(client_df, 'Client')

analysis_results = analyze_data_quality(demograph_df, 'Demograph')

analysis_results = analyze_data_quality(transaction_df, 'Transaction')

analysis_results = analyze_data_quality(creditcard_df, 'CreditCard')

analysis_results = analyze_data_quality(permanentorder_df, 'PermanentOrder')

analysis_results = analyze_data_quality(loan_df, 'Loan')

"""### Enhanced analysis"""

import pandas as pd
import numpy as np
from datetime import datetime
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew, kurtosis, normaltest

def analyze_data_quality(df, table_name, generate_plots=True):
    """
    Enhanced data quality analysis of a DataFrame with advanced checks and visualizations

    Parameters:
    df (pandas.DataFrame): DataFrame to analyze
    table_name (str): Name of the table for reporting
    generate_plots (bool): Whether to generate visualization plots

    Returns:
    dict: Dictionary containing all analysis results
    """
    results = {}

    # 1. Basic Information (keeping original implementation)
    results['basic_info'] = {
        'table_name': table_name,
        'rows': len(df),
        'columns': len(df.columns),
        'memory_usage': df.memory_usage(deep=True).sum() / 1024**2,  # In MB
        'duplicate_rows': df.duplicated().sum()
    }

    # 2. Enhanced Column Analysis
    column_analysis = {}
    for column in df.columns:
        col_stats = {
            # Original statistics
            'dtype': str(df[column].dtype),
            'missing_count': df[column].isnull().sum(),
            'missing_percentage': (df[column].isnull().sum() / len(df)) * 100,
            'unique_count': df[column].nunique(),
            'unique_percentage': (df[column].nunique() / len(df)) * 100,
            'sample_values': list(df[column].value_counts().head(5).index),

            # New advanced statistics
            'outliers': None,
            'skewness': None,
            'kurtosis': None,
            'distribution_type': None
        }

        # Numeric column specific analysis
        if pd.api.types.is_numeric_dtype(df[column]):
            # Basic stats
            col_stats.update({
                'min': df[column].min(),
                'max': df[column].max(),
                'mean': df[column].mean(),
                'std': df[column].std(),
                'median': df[column].median(),
                'zero_count': (df[column] == 0).sum()
            })

            # Advanced statistics
            non_null_data = df[column].dropna()
            if len(non_null_data) > 0:
                # Outlier detection using IQR method
                Q1 = non_null_data.quantile(0.25)
                Q3 = non_null_data.quantile(0.75)
                IQR = Q3 - Q1
                outliers = non_null_data[(non_null_data < (Q1 - 1.5 * IQR)) |
                                       (non_null_data > (Q3 + 1.5 * IQR))]
                col_stats['outliers'] = {
                    'count': len(outliers),
                    'percentage': (len(outliers) / len(non_null_data)) * 100,
                    'sample': list(outliers.head())
                }

                # Distribution analysis
                col_stats['skewness'] = skew(non_null_data)
                col_stats['kurtosis'] = kurtosis(non_null_data)

                # Test for normality
                _, p_value = normaltest(non_null_data)
                col_stats['distribution_type'] = 'Normal' if p_value > 0.05 else 'Non-normal'

        # String column specific analysis
        elif pd.api.types.is_string_dtype(df[column]):
            non_null_data = df[column].dropna()
            col_stats.update({
                'empty_string_count': (non_null_data == '').sum(),
                'min_length': non_null_data.str.len().min(),
                'max_length': non_null_data.str.len().max(),
                'avg_length': non_null_data.str.len().mean(),
                'whitespace_only_count': non_null_data.str.isspace().sum(),
                'special_chars_count': non_null_data.str.contains(r'[^a-zA-Z0-9\s]').sum()
            })

        # Datetime column specific analysis
        elif pd.api.types.is_datetime64_any_dtype(df[column]):
            non_null_data = df[column].dropna()
            if len(non_null_data) > 0:
                col_stats.update({
                    'min_date': non_null_data.min(),
                    'max_date': non_null_data.max(),
                    'date_range_days': (non_null_data.max() - non_null_data.min()).days,
                    'future_dates_count': (non_null_data > pd.Timestamp.now()).sum(),
                    'weekend_dates_count': non_null_data.dt.dayofweek.isin([5, 6]).sum()
                })

        column_analysis[column] = col_stats

    results['column_analysis'] = column_analysis

    # 3. Correlation Analysis (enhanced)
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    if len(numeric_columns) > 1:
        results['correlations'] = {
            'pearson': df[numeric_columns].corr('pearson').to_dict(),
            'spearman': df[numeric_columns].corr('spearman').to_dict()
        }

    # 4. Generate Visualizations
    if generate_plots:
        plots = {}
        sns.set_theme(style="whitegrid")

        # Missing values plot
        plt.figure(figsize=(12, 6))
        missing_data = pd.DataFrame({
            'Column': df.columns,
            'Missing %': [results['column_analysis'][col]['missing_percentage']
                         for col in df.columns]
        })
        sns.barplot(data=missing_data, x='Missing %', y='Column')
        plt.title('Missing Values by Column')
        plots['missing_values'] = plt.gcf()
        plt.close()

        # Distribution plots for numeric columns
        for column in numeric_columns:
            if df[column].nunique() > 1:  # Only plot if there's variation
                plt.figure(figsize=(12, 6))

                # Create subplot for histogram and box plot
                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

                # Histogram with KDE
                sns.histplot(data=df, x=column, kde=True, ax=ax1)
                ax1.set_title(f'Distribution of {column}')

                # Box plot
                sns.boxplot(data=df, x=column, ax=ax2)
                ax2.set_title(f'Box Plot of {column}')

                plt.tight_layout()
                plots[f'distribution_{column}'] = plt.gcf()
                plt.close()

        # Correlation heatmap
        if len(numeric_columns) > 1:
            plt.figure(figsize=(10, 8))
            sns.heatmap(df[numeric_columns].corr(),
                       annot=True,
                       cmap='coolwarm',
                       center=0)
            plt.title('Correlation Heatmap')
            plots['correlation_heatmap'] = plt.gcf()
            plt.close()

        results['plots'] = plots

    # 5. Print Enhanced Summary Report
    print(f"\n{'='*50}")
    print(f"ENHANCED DATA QUALITY ANALYSIS: {table_name}")
    print(f"{'='*50}")

    print("\n1. Basic Information:")
    print(f"Rows: {results['basic_info']['rows']:,}")
    print(f"Columns: {results['basic_info']['columns']}")
    print(f"Memory Usage: {results['basic_info']['memory_usage']:.2f} MB")
    print(f"Duplicate Rows: {results['basic_info']['duplicate_rows']}")

    print("\n2. Column Analysis:")
    for column, stats in results['column_analysis'].items():
        print(f"\n{column}:")
        print(f" - Type: {stats['dtype']}")
        print(f" - Missing: {stats['missing_count']:,} ({stats['missing_percentage']:.2f}%)")
        print(f" - Unique: {stats['unique_count']:,} ({stats['unique_percentage']:.2f}%)")
        print(f" - Sample Values: {stats['sample_values']}")

        if pd.api.types.is_numeric_dtype(df[column]):
            print(f" - Range: [{stats['min']}, {stats['max']}]")
            print(f" - Mean: {stats['mean']:.2f}, Median: {stats['median']:.2f}")
            print(f" - Std: {stats['std']:.2f}")
            if stats['outliers']:
                print(f" - Outliers: {stats['outliers']['count']} ({stats['outliers']['percentage']:.2f}%)")
            print(f" - Distribution: {stats['distribution_type']}")
            print(f" - Skewness: {stats['skewness']:.2f}")
            print(f" - Kurtosis: {stats['kurtosis']:.2f}")

        elif pd.api.types.is_string_dtype(df[column]):
            print(f" - String Lengths: Min={stats['min_length']}, Max={stats['max_length']}, Avg={stats['avg_length']:.2f}")
            print(f" - Empty Strings: {stats['empty_string_count']}")
            print(f" - Whitespace Only: {stats['whitespace_only_count']}")
            print(f" - Special Characters: {stats['special_chars_count']}")

        elif pd.api.types.is_datetime64_any_dtype(df[column]):
            print(f" - Date Range: {stats['min_date']} to {stats['max_date']}")
            print(f" - Range in Days: {stats['date_range_days']}")
            print(f" - Future Dates: {stats['future_dates_count']}")
            print(f" - Weekend Dates: {stats['weekend_dates_count']}")

    return results

# Run the analysis
results = analyze_data_quality(client_df, 'Client')

results['plots']['missing_values'].show()
# this is not working... Why?

results['plots']['correlation_heatmap'].show()
# this is not working... Why?

# Run the analysis
results = analyze_data_quality(loan_df, 'Loan')

# The plots can be accessed via results['plots']
results['plots']['missing_values'].show()
results['plots']['correlation_heatmap'].show()

# Run the analysis
results = analyze_data_quality(client_df, 'Client')

# The plots can be accessed via results['plots']
results['plots']['missing_values'].show()
results['plots']['correlation_heatmap'].show()

# loan_df = pd.read_csv(loan_path, sep=';')
# account_df = pd.read_csv(account_path, sep=';')
# transaction_df = pd.read_csv(transaction_path)
# client_df = pd.read_csv(client_path, sep=';')
# creditcard_df = pd.read_csv(creditcard_path, sep=';')
# demograph_df = pd.read_csv(demograph_path, sep=';')
# disposition_df = pd.read_csv(disposition_path, sep=';')
# permanentorder_df = pd.read_csv(permanentorder_path, sep=';')

# Run the analysis
results = analyze_data_quality(loan_df, 'Loan')

# The plots can be accessed via results['plots']
results['plots']['missing_values'].show()
# results['plots']['correlation_heatmap'].show()

"""## 2. Handling missing values"""

transaction_df['operation'] = transaction_df['operation'].fillna('UNKNOWN_OPERATION')
transaction_df['k_symbol'] = transaction_df['k_symbol'].fillna('UNSPECIFIED_CATEGORY')
analysis_results = analyze_data_quality(transaction_df, 'Transaction')

"""## 3. Feature engineering

### Group 1. TRANSACTION BEHAVIOUR FEATURES
"""



"""### Group 2. INCOME PATTERN FEATURES"""



"""### Group 3. ACCOUNT RELATIONSHIP FEATURES"""



"""### Group 4. CREDIT CARD FEATURES"""



"""### Group 5. PERMANENT ORDER FEATURES"""



"""### Group 6. DEMOGRAPHIC FEATURES

"""



"""### Group 7. RISK INDICATOR FEATURES"""



"""# **End**"""

